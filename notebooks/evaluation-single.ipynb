{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of fold evaluation for EfficientDet\n",
    "\n",
    "Hi everyone!\n",
    "\n",
    "My name is Alex Shonenkov, I am DL/NLP/CV/TS research engineer. Especially I am in Love with NLP & DL.\n",
    "\n",
    "Recently I have created kernels for this competition:\n",
    "- [WBF approach for ensemble](https://www.kaggle.com/shonenkov/wbf-approach-for-ensemble)\n",
    "- [[Training] EfficientDet](https://www.kaggle.com/shonenkov/training-efficientdet)\n",
    "- [[Inference] EfficientDet](https://www.kaggle.com/shonenkov/inference-efficientdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Idea\n",
    "\n",
    "People from ODS slack asked me about competition metrics on my validation split for effdet. So I have decided to publish notebook with calculation oof-score for my models and selection best threshold :) \n",
    "\n",
    "For this aims I have trained 5-folds using published [training kernel](https://www.kaggle.com/shonenkov/training-efficientdet). \n",
    "\n",
    "It is very simple notebook without any really good idea, but I hope It can help you evaluate effdet models in the future during research!\n",
    "So, lets start! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "#!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../efficientdet-pytorch\")\n",
    "sys.path.insert(0, \"../../omegaconf\")\n",
    "sys.path.insert(0, \"../weightedboxesfusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "from effdet.efficientdet import HeadNet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from metrics import calculate_final_score, predict_eval_set, eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/wheat'\n",
    "marking = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
    "    marking[column] = bboxs[:,i]\n",
    "marking.drop(columns=['bbox'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/relevance2-nfs/chec/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "df_folds = marking[['image_id']].copy()\n",
    "df_folds.loc[:, 'bbox_count'] = 1\n",
    "df_folds = df_folds.groupby('image_id').count()\n",
    "df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n",
    "df_folds.loc[:, 'stratify_group'] = np.char.add(\n",
    "    df_folds['source'].values.astype(str),\n",
    "    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",
    ")\n",
    "df_folds.loc[:, 'fold'] = 0\n",
    "\n",
    "for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n",
    "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source      x      y      w      h\n",
       "0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n",
       "1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n",
       "2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n",
       "3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n",
       "4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT_PATH = f'{DATA_DIR}/train'\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, marking, image_ids, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "#                     target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    break\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    validation_dataset = DatasetRetriever(\n",
    "        image_ids=df_folds[df_folds['fold'] == 0].index.values,\n",
    "        marking=marking,\n",
    "        transforms=get_valid_transforms(),\n",
    "        test=True,\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval();\n",
    "    return net.cuda()\n",
    "\n",
    "#models = [\n",
    "    #load_net('./ed5-mixup/best-fold-0.pth'),\n",
    "    #load_net('./ed5-mixup/best-fold-1.pth'),\n",
    "    #load_net('./ed5-mixup/best-fold-2.pth'),\n",
    "    #load_net('../input/effdet5-folds-v2/fold3-best.bin'),\n",
    "    #load_net('../input/effdet5-folds-v2/fold4-best.bin'),\n",
    "#]\n",
    "model = load_net('./ed5-mixup/best-fold-0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of fold prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fd383f187e4edcb285e4692f7f1777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=169.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3285ad8da1c04024b08e27ed1d21f145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_score': 0.684,\n",
       " 'best_threshold': 0.41,\n",
       " 0.2: 0.5944,\n",
       " 0.25: 0.6378,\n",
       " 0.3: 0.663,\n",
       " 0.4: 0.6836,\n",
       " 0.45: 0.6829,\n",
       " 0.5: 0.6758,\n",
       " 0.55: 0.661,\n",
       " 0.6: 0.6348}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = eval_metrics(model, validation_loader)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'boxes': tensor([[ 44.5000, 128.0000, 101.0000, 181.5000],\n",
      "        [108.0000, 141.0000, 170.5000, 200.5000],\n",
      "        [ 98.5000,   0.0000, 139.5000,  17.0000],\n",
      "        [252.0000, 279.0000, 352.0000, 321.0000],\n",
      "        [209.5000, 180.5000, 278.0000, 229.0000],\n",
      "        [192.0000, 121.5000, 238.5000, 163.0000],\n",
      "        [377.0000,  92.0000, 419.0000, 150.0000],\n",
      "        [164.5000, 205.0000, 214.5000, 247.5000],\n",
      "        [218.5000,  20.5000, 256.0000,  61.5000],\n",
      "        [169.0000, 438.5000, 277.0000, 481.0000],\n",
      "        [129.0000,   0.0000, 175.0000,  38.0000],\n",
      "        [137.0000,  31.5000, 194.5000,  83.0000],\n",
      "        [437.5000, 389.0000, 512.0000, 425.5000],\n",
      "        [354.0000, 293.5000, 399.0000, 333.5000],\n",
      "        [ 64.0000, 181.5000, 107.5000, 218.5000],\n",
      "        [405.5000, 480.0000, 486.0000, 512.0000],\n",
      "        [359.5000,   0.0000, 403.5000,  31.5000],\n",
      "        [200.5000,  72.5000, 250.5000, 114.5000],\n",
      "        [264.5000,  44.5000, 309.5000,  82.0000],\n",
      "        [ 38.0000, 375.0000, 115.0000, 418.5000],\n",
      "        [433.5000, 336.0000, 511.0000, 393.5000],\n",
      "        [473.0000, 228.5000, 512.0000, 256.5000],\n",
      "        [469.5000, 426.0000, 510.5000, 495.0000],\n",
      "        [ 65.0000,  92.0000, 101.0000, 129.0000],\n",
      "        [  0.0000, 145.5000,  24.5000, 177.5000],\n",
      "        [175.0000, 345.5000, 216.0000, 382.5000],\n",
      "        [142.0000, 314.5000, 180.0000, 366.0000],\n",
      "        [  0.0000, 325.0000,  47.0000, 377.5000],\n",
      "        [  0.0000, 307.0000,  20.0000, 348.0000],\n",
      "        [ 35.5000, 331.5000,  67.5000, 380.0000],\n",
      "        [ 92.5000, 326.0000, 121.5000, 357.0000],\n",
      "        [235.5000, 476.0000, 273.5000, 512.0000],\n",
      "        [ 99.5000, 348.0000, 158.5000, 386.5000],\n",
      "        [480.0000, 253.0000, 512.0000, 304.5000],\n",
      "        [119.0000, 478.5000, 158.5000, 512.0000],\n",
      "        [182.5000,  30.5000, 208.0000,  61.0000],\n",
      "        [239.0000,   0.0000, 279.5000,  23.5000],\n",
      "        [402.0000, 407.5000, 434.5000, 445.0000],\n",
      "        [357.5000, 429.5000, 391.5000, 463.0000],\n",
      "        [484.0000, 192.0000, 512.0000, 229.5000],\n",
      "        [446.5000,  52.5000, 492.5000,  90.5000]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([0])}, {'boxes': tensor([[  0.0000,  51.5000,  11.0000,  87.5000],\n",
      "        [  0.0000,   4.0000,  38.0000,  49.0000],\n",
      "        [ 29.5000,  32.0000,  60.0000,  62.0000],\n",
      "        [ 26.0000,  56.0000,  71.0000, 100.5000],\n",
      "        [ 55.5000, 113.5000, 103.0000, 177.0000],\n",
      "        [122.0000, 102.0000, 169.5000, 161.5000],\n",
      "        [  8.0000, 201.0000,  64.5000, 288.5000],\n",
      "        [ 81.5000, 183.5000, 132.5000, 236.0000],\n",
      "        [ 11.0000, 301.5000,  49.5000, 380.0000],\n",
      "        [ 83.0000, 376.0000, 140.5000, 452.5000],\n",
      "        [  1.5000, 434.0000,  39.0000, 460.5000],\n",
      "        [ 46.0000, 469.0000, 103.5000, 502.0000],\n",
      "        [ 19.5000, 504.5000,  59.0000, 512.0000],\n",
      "        [ 78.5000, 498.0000, 120.0000, 512.0000],\n",
      "        [115.5000, 492.0000, 147.0000, 512.0000],\n",
      "        [393.0000, 459.0000, 438.0000, 512.0000],\n",
      "        [362.5000, 504.5000, 388.0000, 512.0000],\n",
      "        [278.0000, 443.0000, 324.0000, 472.5000],\n",
      "        [161.5000, 451.5000, 209.5000, 512.0000],\n",
      "        [157.5000, 310.0000, 231.0000, 384.0000],\n",
      "        [226.0000, 395.0000, 273.0000, 445.0000],\n",
      "        [268.0000, 351.0000, 334.5000, 443.5000],\n",
      "        [304.0000, 335.5000, 344.0000, 394.5000],\n",
      "        [348.0000, 344.0000, 389.5000, 384.5000],\n",
      "        [359.5000, 375.5000, 392.0000, 410.0000],\n",
      "        [384.5000, 393.5000, 426.5000, 423.0000],\n",
      "        [462.0000, 346.0000, 512.0000, 372.5000],\n",
      "        [503.5000, 369.0000, 512.0000, 401.5000],\n",
      "        [416.0000, 295.0000, 457.0000, 330.5000],\n",
      "        [357.5000, 265.0000, 397.5000, 324.0000],\n",
      "        [270.0000, 236.0000, 335.5000, 285.5000],\n",
      "        [191.0000, 189.5000, 226.0000, 250.0000],\n",
      "        [189.5000, 148.5000, 247.5000, 188.0000],\n",
      "        [359.5000,  97.5000, 418.5000, 145.0000],\n",
      "        [253.5000,  73.0000, 283.5000, 117.0000],\n",
      "        [224.0000,  31.0000, 296.0000,  82.0000],\n",
      "        [165.5000,  47.5000, 231.5000,  78.5000],\n",
      "        [322.5000,  10.5000, 370.0000,  57.5000],\n",
      "        [458.5000,   1.5000, 506.0000,  57.5000],\n",
      "        [435.5000, 225.5000, 486.0000, 276.5000],\n",
      "        [ 13.0000,   0.0000,  94.5000,  14.5000],\n",
      "        [118.0000,  16.0000, 162.5000,  50.5000],\n",
      "        [343.5000,  60.0000, 406.5000,  96.0000]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([1])}, {'boxes': tensor([[161.0000,  23.0000, 206.0000,  96.0000],\n",
      "        [257.0000, 108.5000, 302.0000, 152.0000],\n",
      "        [293.0000,   8.5000, 354.5000,  66.5000],\n",
      "        [161.0000, 453.0000, 201.5000, 489.5000],\n",
      "        [218.5000, 207.0000, 291.5000, 275.0000],\n",
      "        [  0.0000, 172.5000,  57.5000, 211.0000],\n",
      "        [485.0000, 107.5000, 512.0000, 154.5000],\n",
      "        [277.5000, 262.0000, 350.5000, 320.0000],\n",
      "        [133.0000, 135.5000, 183.0000, 203.5000],\n",
      "        [165.0000, 252.0000, 213.5000, 313.5000],\n",
      "        [224.0000, 428.5000, 264.5000, 482.5000],\n",
      "        [280.0000, 353.0000, 349.0000, 427.5000],\n",
      "        [257.0000,   7.5000, 296.5000,  66.5000],\n",
      "        [103.5000, 270.5000, 152.5000, 314.0000],\n",
      "        [380.0000, 223.0000, 422.0000, 263.0000],\n",
      "        [417.0000, 345.5000, 469.5000, 400.5000],\n",
      "        [226.5000,  19.0000, 264.5000,  87.0000],\n",
      "        [449.0000, 253.0000, 490.0000, 288.0000],\n",
      "        [205.0000,   0.0000, 244.5000,  25.0000],\n",
      "        [ 94.5000,   0.0000, 124.0000,  27.5000],\n",
      "        [360.0000,   0.0000, 408.0000,  26.0000]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([2])}, {'boxes': tensor([[3.3100e+02, 8.7500e+01, 3.7750e+02, 1.4100e+02],\n",
      "        [2.7800e+02, 1.6250e+02, 3.1550e+02, 1.9550e+02],\n",
      "        [2.1750e+02, 2.9650e+02, 2.6450e+02, 3.5650e+02],\n",
      "        [1.6850e+02, 1.9650e+02, 2.3500e+02, 2.4100e+02],\n",
      "        [1.1600e+02, 1.0900e+02, 1.9550e+02, 1.6750e+02],\n",
      "        [4.3000e+01, 4.8000e+01, 8.8500e+01, 1.1300e+02],\n",
      "        [5.0000e-01, 1.3650e+02, 3.5000e+01, 1.6950e+02],\n",
      "        [4.3300e+02, 1.4400e+02, 4.7400e+02, 1.8650e+02],\n",
      "        [4.1850e+02, 4.1300e+02, 4.8000e+02, 4.8000e+02],\n",
      "        [1.4400e+02, 4.3000e+02, 2.1850e+02, 4.8250e+02],\n",
      "        [1.1150e+02, 6.7500e+01, 1.3200e+02, 9.2000e+01],\n",
      "        [1.1050e+02, 2.1150e+02, 1.6950e+02, 2.5600e+02],\n",
      "        [3.8100e+02, 1.3000e+02, 4.2150e+02, 1.6900e+02],\n",
      "        [1.3000e+02, 2.7950e+02, 1.7550e+02, 3.1350e+02],\n",
      "        [1.7850e+02, 2.8250e+02, 2.1500e+02, 3.1200e+02],\n",
      "        [3.4500e+01, 2.7550e+02, 8.4000e+01, 3.0200e+02],\n",
      "        [2.0500e+01, 3.3200e+02, 1.3950e+02, 3.9950e+02],\n",
      "        [8.2500e+01, 3.6700e+02, 1.3800e+02, 3.9800e+02],\n",
      "        [2.5000e+02, 3.5100e+02, 2.9000e+02, 4.1050e+02],\n",
      "        [3.4550e+02, 3.0550e+02, 3.8500e+02, 3.2600e+02],\n",
      "        [3.5800e+02, 3.3250e+02, 3.8600e+02, 3.6000e+02],\n",
      "        [4.0900e+02, 3.0500e+02, 4.4350e+02, 3.5850e+02],\n",
      "        [3.8750e+02, 2.3250e+02, 4.5050e+02, 2.7250e+02],\n",
      "        [4.1500e+02, 1.8750e+02, 4.5550e+02, 2.1450e+02],\n",
      "        [4.7350e+02, 2.1050e+02, 5.0250e+02, 2.2900e+02],\n",
      "        [3.8900e+02, 4.9200e+02, 4.7450e+02, 5.1200e+02],\n",
      "        [9.8500e+01, 4.5800e+02, 1.3600e+02, 5.0000e+02],\n",
      "        [0.0000e+00, 4.3500e+02, 2.4000e+01, 4.7100e+02]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'image_id': tensor([3])})\n"
     ]
    }
   ],
   "source": [
    "for img, target, img_id in validation_loader:\n",
    "    print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
